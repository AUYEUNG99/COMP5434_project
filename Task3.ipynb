{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Train_Data.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainData, testData = train_test_split(data, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3994    4\n",
      "423     2\n",
      "2991    3\n",
      "1221    1\n",
      "506     1\n",
      "       ..\n",
      "1130    1\n",
      "1294    4\n",
      "860     4\n",
      "3507    4\n",
      "3174    1\n",
      "Name: price range, Length: 3200, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(trainData['price range'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3,\n",
    "                      stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3,\n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel * self.expansion, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)#open shortcut connection\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=5):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 3\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size=2, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 3,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 6, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 12, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 24, 2, stride=2)\n",
    "        self.fc1 = nn.Linear(24, 12)\n",
    "        self.fc2 = nn.Linear(12, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)  # strides=[1,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.layer1(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.layer2(out)\n",
    "       # out = self.dropout(out)\n",
    "        out = self.layer3(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out,1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=(2,2),stride=1,padding=1)\n",
    "        self.pool1 =nn.MaxPool2d(2,2)\n",
    "        #self.pool2  = nn.Maxpool2d(1,2)\n",
    "        #self.conv2 = nn.Conv2d(10, 20, kernel_size=(1,1))\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.fc1 = nn.Linear(24,12)\n",
    "        #self.fc2 = nn.Linear(16,8)\n",
    "        self.fc2 = nn.Linear(12,5)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        #x = self.pool2(F.relu(self.conv2(x)))\n",
    "        #x = x.view(-1, 16 * 1 *1 )\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        #x = F.softmax(self.fc2(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def dirichlet_partition(training_data, testing_data, alpha, user_num):\n",
    "    idxs_train = np.arange(len(training_data))\n",
    "    idxs_valid = np.arange(len(testing_data))\n",
    "\n",
    "    if hasattr(training_data, 'price range'):\n",
    "        labels_train = training_data['price range']\n",
    "        labels_valid = testing_data['price range']\n",
    "    elif hasattr(training_data, 'img_label'):\n",
    "        labels_train = training_data.img_label\n",
    "        labels_valid = testing_data.img_label\n",
    "\n",
    "    idxs_labels_train = np.vstack((idxs_train, labels_train))\n",
    "    # print(\"idxs_labels_train 1: \", idxs_labels_train)\n",
    "    idxs_labels_train = idxs_labels_train[:, idxs_labels_train[1,:].argsort()]\n",
    "    # print(\"idxs_labels_train 2: \", idxs_labels_train)\n",
    "    idxs_labels_valid = np.vstack((idxs_valid, labels_valid))\n",
    "    idxs_labels_valid = idxs_labels_valid[:, idxs_labels_valid[1,:].argsort()]\n",
    "\n",
    "    labels = np.unique(labels_train, axis=0)\n",
    "\n",
    "    data_train_dict = data_organize(idxs_labels_train, labels)\n",
    "    # print(\"data_train_dict[0]: \", data_train_dict[0])\n",
    "    # print(\"data_train_dict[9]: \", data_train_dict[9])\n",
    "    data_valid_dict = data_organize(idxs_labels_valid, labels)\n",
    "\n",
    "    data_partition_profile_train = {}\n",
    "    data_partition_profile_valid = {}\n",
    "\n",
    "\n",
    "    for i in range(user_num):\n",
    "        data_partition_profile_train[i] = []\n",
    "        data_partition_profile_valid[i] = []\n",
    "\n",
    "    ## Distribute rest data\n",
    "    for label in data_train_dict:\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, user_num))\n",
    "        proportions_train = len(data_train_dict[label])*proportions\n",
    "        proportions_valid = len(data_valid_dict[label]) * proportions\n",
    "\n",
    "        for user in data_partition_profile_train:\n",
    "\n",
    "            data_partition_profile_train[user]   \\\n",
    "                = set.union(set(np.random.choice(data_train_dict[label], int(proportions_train[user]) , replace = False)), data_partition_profile_train[user])\n",
    "            data_train_dict[label] = list(set(data_train_dict[label])-data_partition_profile_train[user])\n",
    "\n",
    "\n",
    "            data_partition_profile_valid[user] = set.union(set(\n",
    "                np.random.choice(data_valid_dict[label], int(proportions_valid[user]),\n",
    "                                 replace=False)), data_partition_profile_valid[user])\n",
    "            data_valid_dict[label] = list(set(data_valid_dict[label]) - data_partition_profile_valid[user])\n",
    "\n",
    "\n",
    "        while len(data_train_dict[label]) != 0:\n",
    "            rest_data = data_train_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_train[user].add(rest_data)\n",
    "            data_train_dict[label].remove(rest_data)\n",
    "\n",
    "        while len(data_valid_dict[label]) != 0:\n",
    "            rest_data = data_valid_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_valid[user].add(rest_data)\n",
    "            data_valid_dict[label].remove(rest_data)\n",
    "\n",
    "    for user in data_partition_profile_train:\n",
    "        data_partition_profile_train[user] = list(data_partition_profile_train[user])\n",
    "        data_partition_profile_valid[user] = list(data_partition_profile_valid[user])\n",
    "        np.random.shuffle(data_partition_profile_train[user])\n",
    "        np.random.shuffle(data_partition_profile_valid[user])\n",
    "\n",
    "    return data_partition_profile_train, data_partition_profile_valid\n",
    "\n",
    "\n",
    "def data_organize(idxs_labels, labels):\n",
    "    data_dict = {}\n",
    "\n",
    "    labels = np.unique(labels, axis=0)\n",
    "    for one in labels:\n",
    "        data_dict[one] = []\n",
    "\n",
    "    for i in range(len(idxs_labels[1, :])):\n",
    "        data_dict[idxs_labels[1, i]].append(idxs_labels[0, i])\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index[0]： [3196, 1903, 2602, 790, 3019, 366, 1121, 2151, 3045, 2761, 2143, 429, 2213, 2828, 2815, 1246, 1002, 2625, 2073, 2389, 307, 2450, 1862, 2412, 2579, 2685, 3157, 2923, 2077, 984, 2559, 163, 2593, 2091, 1077, 1554, 2590, 2558, 2939, 1043, 2270, 1345, 623, 1650, 1041, 2565, 1811, 96, 2817, 2365, 181, 75, 1538, 1599, 1249, 106, 586, 278, 430, 1151, 3039, 850, 141, 337, 2763, 1780, 543, 1214, 471, 316, 1908, 1268, 1283, 1669, 3068, 1819, 8, 156, 793, 2300, 2573, 1425, 1147, 924, 1936, 2826, 1229, 2706, 1937, 770, 250, 1556, 1134, 2141, 2194, 1356, 2388, 2234, 1529, 2122, 1078, 2529, 426, 646, 933, 2320, 1612, 1563, 1536, 2186, 2536, 121, 1372, 256, 1648, 2037, 1462, 299, 2836, 928, 1899, 1967, 2459, 2264, 196, 2628, 411, 1274, 1932, 2837, 3077, 1726, 1052, 2608, 340, 1508, 2714, 1288, 371, 1255, 1641, 2657, 2702, 621, 2423, 2525, 18, 1939, 1097, 1140, 1296, 1501, 61, 249, 3084, 1740, 2516, 3113, 2493, 1009, 913, 2279, 2313, 1209, 2523, 2781, 867, 2638, 938, 1399, 730, 918, 2513, 1389, 2549, 282, 1120, 691, 2311, 1818, 2206, 2314, 2760, 2809, 2197, 261, 936, 1817, 1098, 1546, 2396, 1074, 230, 2784, 854, 2281, 2778, 2222, 1602, 125, 1036, 542, 479, 906, 813, 1336, 315, 2589, 3021, 2029, 1319, 1407, 1995, 238, 1735, 259, 1827, 2564, 1460, 2705, 567, 358, 260, 1632, 2719, 2385, 384, 896, 76, 591, 2637, 2547, 2799, 288, 1574, 2268, 1293, 1993, 2491, 3195, 2796, 2652, 1365, 1979, 1247, 2896, 2336, 3034, 1483, 709, 180, 2762, 2369, 2348, 2713, 2347, 1800, 809, 1484, 2510, 1542, 1173, 560, 860, 1938, 280, 2373, 321, 996, 2905, 1587, 482, 329, 2172, 468, 1618, 2319, 186, 573, 2207, 328, 830, 1018, 821, 849, 2957, 207, 1610, 1575, 1569, 1494, 1868, 3081, 1727, 335, 1451, 161, 759, 889, 3188, 1325, 1956, 705, 2782, 2647, 1756, 2609, 819, 2745, 2238, 1964, 2747, 397, 2768, 1991, 2183, 1590, 465, 209, 1183, 3023, 488, 3170, 2317, 9, 87, 572, 2998, 1468, 160, 2664, 1421, 2631, 1511, 1970, 1880, 781, 101, 2888, 2592, 2750, 287, 489, 2429, 330, 2511, 434, 152, 2363, 2979, 2938, 297, 717, 654, 1327, 505, 822, 72, 1318, 2919, 540, 1158, 971, 1403, 674, 2506, 14, 2899, 3005, 2982, 3059, 1170, 1745, 2384, 2988, 78, 1799, 719, 3080, 882, 2644, 2327, 975, 1878, 79, 1083, 2622, 968, 1820, 641, 870, 3135, 2718, 1860, 2196, 1195, 224, 588, 2165, 713, 1025, 2655, 2960, 1826, 1977, 1019, 3172, 3048, 2619, 2240, 2616, 907, 1028, 754, 1738, 1674, 2855, 582, 82, 1984, 452, 1794, 3134, 2953, 831, 2972, 2011, 145, 88, 1744, 258, 73, 2913, 2654, 1235, 1614, 3183, 1367, 42, 90, 845, 2024, 2843, 2942, 1057, 1915, 1807, 1949, 955, 1994, 5, 91, 682, 1148, 1244, 2686, 3114, 1861, 1922, 2503, 2350, 977, 2474, 1685, 1865, 3105, 765, 1066, 1838, 2838, 1266, 2204, 1017, 1990, 979, 3070, 511, 2050, 497, 1679, 295, 1586, 1806, 300, 2771, 3010, 140, 1216, 2367, 3008, 2859, 120, 290, 98, 772, 1881, 2417, 1692, 2891, 607, 656, 2221, 1700, 1925, 1184, 2931, 3144, 2539, 368, 2116, 1382, 1400, 462, 1051, 773, 1844, 1433, 2176, 2893, 168, 1190, 1712, 1691, 1472, 1849, 2676, 243, 2744, 2802, 2371, 2737, 2980, 3000, 652, 2227, 635, 2249, 244, 1432, 872, 1548, 2124, 706, 444, 1320, 1874, 2062, 1523, 514, 2056, 403, 1768, 3199, 1534, 2078, 1512, 1652, 1349, 266, 433, 3173, 2422, 937, 2542, 2284, 575, 1577, 2897, 923, 17, 1328, 1532, 622, 1673, 1475, 2269, 2618, 1539, 2257, 1251, 2216, 1206, 2457, 1983, 912, 750, 777, 137, 546, 1750, 265, 218, 2155, 1185, 398, 3015, 632, 1401, 428, 3179, 2460, 1631, 1985, 1429, 1747, 764, 2242, 1290, 2058, 2036, 2166, 2568, 1227, 1226, 154, 2793, 2425, 1891, 530, 1771, 1125, 1540, 1816, 2918, 97, 1873, 1711, 838, 2964, 3171, 1686, 2611, 3025, 2874, 438, 2043, 122, 2921, 1902, 277, 1323, 2002, 1447, 686, 1758, 1191, 1841, 527, 1260, 2495, 1139, 2765, 496, 945, 942, 2395, 953, 1793, 370, 1363, 1896, 2211, 2209, 2736, 2027, 2671, 2950, 2200, 900, 1359, 501, 2226, 1755, 211, 1080, 2377, 2621, 367, 216, 2335, 1760, 2560, 294, 864, 1588, 909, 306, 2192, 1207, 3148, 780, 1905, 2504, 2689, 1415, 1032, 356, 1303, 2853, 2208, 595, 1882, 375, 2416, 1787, 1166, 1764, 333, 2470, 745, 1259, 3035, 401, 687, 1976, 2580, 2967, 3082, 1276, 558, 2981, 60, 2302, 2045, 3142, 1085, 464, 609, 1478, 956, 291, 2518, 1822, 2097, 3012, 1634, 222, 2471, 2963, 2420, 1219, 771, 2126, 1072, 2658, 2076, 2405, 182, 3164, 1576, 1081, 2656, 1947, 1388, 3127, 853, 1480, 2604, 4, 1528, 2428, 766, 1281, 767, 1694, 176, 257, 2800, 539, 1215, 286, 784, 2808, 80, 2038, 342, 796, 1011, 2304, 806, 695, 1308, 1160, 1952, 317, 2507, 2243, 420, 768, 1786, 564, 2787, 1150, 1311, 1558, 283, 236, 2082, 3, 3125, 1141, 1839, 1853, 3133, 242, 1595, 1628, 1579, 1294, 692, 1471, 1204, 32, 451, 3089, 3063, 1732, 2733, 1387, 2401, 1927, 3122, 1698, 2575, 1071, 343, 2361, 274, 50, 1427, 327, 3037, 736, 1099, 1626, 2349, 11, 734, 1734, 1233, 351, 1394, 2421, 1622, 1701, 2490, 848, 2517, 762, 778, 2917, 336, 1987, 2779, 525, 104, 544, 3071, 1644, 788, 1103, 2329, 1330, 3106, 214, 341, 1617, 99, 2214, 2331, 1809, 948, 1572, 1144, 2307, 487, 123, 1189, 71, 2463, 355, 3138, 801, 1230, 2951, 1479, 2479, 2473, 683, 2585, 143, 2540, 1570, 1361, 472, 1373, 528, 2008, 1642, 3056, 818, 1347, 1302, 2772, 184, 688, 3049, 2553, 3190, 7, 2649, 1157, 1553, 832, 3054, 1476, 2309, 1854, 2316, 967, 2924, 1397, 2577, 1385, 2712, 2910, 1693, 533, 1926, 1759, 2832, 2051, 2404, 3032, 1371, 2880, 1547, 453, 1857, 1208, 508, 2804, 354, 30, 1843, 1703, 2734, 2280, 1054]\n",
      "train_index[1]： [1960, 386, 833, 2015, 753, 445, 620, 3187, 2670, 1254, 1507, 2872, 2666, 1201, 292, 1374, 934, 2708, 2160, 2766, 658, 1655, 2948, 1029, 2170, 1651, 966, 2830, 1681, 989, 2704, 2229, 2663, 2096, 1968, 3194, 3140, 2717, 2060, 1391, 679, 1790, 139, 2047, 2033, 1682, 904, 776, 2812, 2505, 707, 2455, 2431, 3075, 2492, 1684, 2742, 1521, 2531, 2863, 2283, 524, 2070, 2900, 1537, 234, 2321, 31, 3060, 2659, 3155, 2230, 2017, 2039, 2372, 2871, 2583, 1333, 2840, 2400, 603, 1721, 2477, 1114, 2937, 2774, 797, 2198, 2610, 592, 611, 279, 485, 203, 2770, 1118, 515, 565, 1661, 841, 2387, 1315, 1146, 2177, 1603, 3086, 1305, 2596, 193, 972, 1998, 2255, 1957, 3011, 2820, 2338, 2681, 268, 1161, 402, 2241, 1549, 2995, 47, 1267, 512, 2992, 1524, 494, 134, 2847, 1500, 2816, 2275, 2915, 46, 1109, 334, 670, 2480, 2512, 440, 2084, 349, 2727, 893, 1314, 507, 1824, 320, 1477, 2753, 2502, 2976, 83, 1751, 164, 2190, 2572, 100, 476, 1583, 419, 1890, 2729, 626, 2010, 2040, 741, 2035, 2109, 2380, 102, 1497, 237, 2810, 2819, 2876, 1728, 614, 2272, 1754, 1424, 3020, 1869, 2355, 881, 1458, 1781, 374, 702, 2648, 2034, 2738, 162, 684, 470, 2250, 74, 2973, 190, 2725, 642, 240, 2723, 602, 1053, 1188, 2048, 1306, 1680, 1832, 25, 678, 1213, 1231, 2632, 625, 353, 1297, 1513, 2892, 3123, 2758, 2606, 2623, 303, 2449, 2541, 1777, 2827, 1453, 416, 2322, 2651, 939, 585, 2703, 311, 2545, 225, 2806, 2179, 2662, 2462, 2786, 980, 2432, 1489, 673, 2067, 2107, 689, 3066, 1909, 1381, 114, 389, 2599, 1271, 1428, 2278, 960, 2482, 289, 2068, 1654, 1516, 2212, 892, 2543, 1616, 2528, 2934, 1358, 880, 1774, 1823, 263, 2875, 2574, 2595, 1346, 1049, 653, 2977, 672, 1550, 1653, 677, 726, 2184, 2292, 1855, 1935, 655, 599, 13, 1638, 1492, 3162, 1895, 1591, 2999, 943, 2434, 522, 1465, 1978, 737, 2418, 1366, 1024, 577, 3118, 1951, 2219, 2767, 108, 2340, 3069, 1668, 2228, 628, 600, 3176, 538, 2044, 2159, 725, 1505, 188, 2394, 2532, 302, 3150, 2472, 2169, 2352, 2261, 1954, 1113, 2131, 2262, 376, 1067, 3137, 212, 475, 2895, 2551, 1544, 424, 1859, 1253, 803, 2414, 852, 2860, 842, 2797, 1179, 2344, 999, 721, 1334, 48, 1130, 1555, 718, 1470, 113, 2202, 2020, 637, 2850, 1852, 593, 107, 2903, 2087, 2436, 1419, 210, 1672, 2375, 787, 714, 1086, 2013, 2318, 2612, 1102, 59, 2624, 1490, 1871, 200, 769, 1792, 109, 22, 1412, 715, 248, 3167, 2785, 1842, 1506, 425, 643, 640, 1091, 551, 2699, 513, 2886, 1864, 2908, 3098, 2146, 1930, 2282, 1379, 2775, 324, 978, 991, 373, 1459, 2598, 308, 1220, 616, 774, 2232, 1169, 1104, 1068, 871, 685, 1597, 861, 118, 2427, 2120, 447, 954, 3111, 1509, 2295, 3057, 2030, 1174, 629, 3108, 272, 157, 3009, 1261, 1030, 792, 1757, 926, 1285, 255, 1830, 441, 3120, 563, 545, 1332, 1073, 151, 1962, 2248, 839, 877, 3067, 3143, 52, 1552, 1625, 432, 220, 761, 2135, 2128, 500, 783, 3101, 227, 2101, 3102, 1627, 3181, 57, 1941, 2330, 1063, 941, 1872, 276, 463, 2339, 2522, 232, 1675, 2677, 1196, 284, 2157, 905, 1162, 1279, 127, 829, 1637, 2966, 388, 1958, 1022, 2926, 318, 2481, 1167, 1683, 1611, 1280, 1200, 894, 2849, 331, 1741, 1234, 1042, 1242, 3107, 638, 2741, 304, 700, 1718, 45, 2645, 3017, 3002, 1165, 1122, 1885, 1369, 2102, 828, 1107, 378, 1770, 3087, 3033, 724, 1088, 863, 2061, 2012, 915, 1559, 1269, 348, 2629, 1607, 2993, 1749, 3036, 2879, 431, 24, 2115, 2447, 661, 2014, 457, 1722, 2199, 578, 1313, 1834, 3040, 816, 925, 3027, 2486, 3189, 885, 969, 1177, 2732, 855, 1573, 2907, 360, 2679, 1592, 2374, 703, 735, 1879, 1223, 381, 2168, 12, 3022, 2237, 879, 619, 264, 269, 2026, 1753, 866, 2000, 1914, 2821, 2484, 950]\n"
     ]
    }
   ],
   "source": [
    "user_num = 4\n",
    "alpha = 0.5\n",
    "train_index, test_index = dirichlet_partition(trainData, testData, alpha=alpha, user_num=user_num)\n",
    "print(\"train_index[0]：\", train_index[0])\n",
    "print(\"train_index[1]：\", train_index[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(DatasetSplit(trainData, train_index[user_index]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DatasetSplit object at 0x000002022785AA00>\n"
     ]
    }
   ],
   "source": [
    "print(train_data_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def local_trainer(dataset, model, global_round, device, local_epoch, batchsize):\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    global_round, iter, batch_idx * len(images),\n",
    "                    len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def local_trainer(dataset, net, global_round, device, local_epoch, batchsize,testloader):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    trainloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    testloader = DataLoader(testloader, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    for epoch in range(local_epoch):\n",
    "        running_loss = 0.0\n",
    "        total=0\n",
    "        correct=0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            #optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(predicted)\n",
    "            #print(labels)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f'train accuracy = {100 * correct / total:.2f}%')\n",
    "        # Test the model on the test set\n",
    "        # Test the model on the test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                outputs = net(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                #print(outputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                #print(predicted)\n",
    "                #print(labels)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_loss = running_loss / len(testloader)\n",
    "        test_losses.append(test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def inference(model, testloader):\n",
    "    \"\"\" Returns the inference accuracy and loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(testloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "    loss /= batch_idx\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key].float(), len(w))\n",
    "    return w_avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "global_model = Net().to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2855",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 2855",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m global_acc \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m user_index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(user_num):\n\u001B[1;32m----> 9\u001B[0m     model_weights, loss \u001B[38;5;241m=\u001B[39m \u001B[43mlocal_trainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43muser_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mglobal_model\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mround_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtestData\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     local_weights\u001B[38;5;241m.\u001B[39mappend(copy\u001B[38;5;241m.\u001B[39mdeepcopy(model_weights))\n\u001B[0;32m     11\u001B[0m     local_losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mlocal_trainer\u001B[1;34m(dataset, net, global_round, device, local_epoch, batchsize, testloader)\u001B[0m\n\u001B[0;32m     10\u001B[0m total\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     11\u001B[0m correct\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trainloader):\n\u001B[0;32m     13\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mDatasetSplit.__getitem__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, item):\n\u001B[1;32m---> 13\u001B[0m     image, label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43midxs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(image), torch\u001B[38;5;241m.\u001B[39mtensor(label)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 2855"
     ]
    }
   ],
   "source": [
    "global_rounds = 5\n",
    "local_epochs = 5\n",
    "for round_idx in range(global_rounds):\n",
    "    local_weights = []\n",
    "    local_losses = []\n",
    "    global_acc = []\n",
    "\n",
    "    for user_index in range(user_num):\n",
    "        model_weights, loss = local_trainer(train_data_list[user_index], copy.deepcopy(global_model), round_idx, device, local_epochs, batch_size,testData)\n",
    "        local_weights.append(copy.deepcopy(model_weights))\n",
    "        local_losses.append(loss)\n",
    "\n",
    "    global_weight = average_weights(local_weights)\n",
    "    global_model.load_state_dict(global_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}