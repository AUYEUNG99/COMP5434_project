{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TQBNFE8oNGQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class HousepriceDataset(Dataset):\n",
        "    def __init__(self, data, city, label, zipcodes, train_num):\n",
        "        super(HousepriceDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.city = city\n",
        "        self.zipcodes = zipcodes\n",
        "        self.label = label\n",
        "        self.num = train_num\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label) - self.num\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        data = torch.Tensor(self.data[index])\n",
        "        city = torch.tensor(self.city[index], dtype=torch.int)\n",
        "        label = torch.tensor(self.label[index], dtype=torch.long)\n",
        "        \"\"\"\n",
        "\n",
        "        data = torch.Tensor(self.data[index:index + self.num])\n",
        "        city = torch.tensor(self.city[index:index + self.num], dtype=torch.int)\n",
        "        zipcodes = torch.tensor(self.zipcodes[index:index + self.num], dtype=torch.int)\n",
        "        label = torch.tensor(self.label[index:index + self.num], dtype=torch.long)\n",
        "\n",
        "        return data, city, zipcodes, label\n",
        "\n",
        "class ValidationDataset(Dataset):\n",
        "    def __init__(self, data, city, label, train_num):\n",
        "        super(ValidationDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.city = city\n",
        "        self.label = label\n",
        "        self.num = train_num\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label) // self.num\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        data = torch.Tensor(self.data[index])\n",
        "        city = torch.tensor(self.city[index], dtype=torch.int)\n",
        "        label = torch.tensor(self.label[index], dtype=torch.long)\n",
        "        \"\"\"\n",
        "\n",
        "        data = torch.Tensor(self.data[index * self.num:index * self.num + self.num])\n",
        "        city = torch.tensor(self.city[index * self.num:index * self.num + self.num], dtype=torch.int)\n",
        "        label = torch.tensor(self.label[index * self.num:index * self.num + self.num], dtype=torch.long)\n",
        "\n",
        "        return data, city, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, model_dim, hidden_dim, dropout=0.1):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        # for simplicity, set Value dimension = hidden_dim\n",
        "        self.linear = nn.Linear(model_dim, hidden_dim * 3)\n",
        "        # for numerical stable\n",
        "        self.temperature = model_dim ** 0.5\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # the idea of Attention: input shape = output shape\n",
        "        self.fc = nn.Linear(hidden_dim, model_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: [batch, 100, model_dim]\n",
        "        :return: new features after attention\n",
        "        \"\"\"\n",
        "        q, k, v = torch.chunk(self.linear(x), chunks=3, dim=-1)  # all in shape [Batch, 100, hidden_dim]\n",
        "        residual = x\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) / self.temperature\n",
        "        \"\"\"\n",
        "        mask operation waited to be implemented\n",
        "        \"\"\"\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        output = self.fc(output) + residual\n",
        "        # print(output.shape)\n",
        "        output = torch.permute(output, [0, 2, 1])  # in form [Batch, features(channels), sequence_len] to use BatchNorm\n",
        "        return output\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, d_model, embed_dim, hidden_dim, city_num, num_classes=4):\n",
        "        \"\"\"\n",
        "        :param d_model:        original model dimension before embedding for cities\n",
        "        :param embed_dim:      embedding dim for cities (district)\n",
        "        :param hidden_dim:     hidden dim we want for Attention operation\n",
        "        :param city_num:       unique cities in dataset\n",
        "        \"\"\"\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        self.hidden = hidden_dim\n",
        "        self.d_model = d_model\n",
        "        self.city_embed = nn.Embedding(city_num, embed_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(embed_dim + d_model, 2 * hidden_dim)\n",
        "        self.attn1 = Attention(2 * hidden_dim, 2 * hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(2 * hidden_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(2 * hidden_dim, 3 * hidden_dim)\n",
        "        self.attn2 = Attention(3 * hidden_dim, 2 * hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(3 * hidden_dim)\n",
        "\n",
        "        self.activ = nn.ReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(3 * hidden_dim, 2 * hidden_dim)\n",
        "        self.bn4 = nn.BatchNorm1d(2 * hidden_dim)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cities):\n",
        "        \"\"\"\n",
        "        :param x:            house data input except cities (zipcode & district are abandoned)      [Batch, num, dim]\n",
        "        :param cities:       numeric value of original city attribute                               [Batch, num, embed]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        1. embed the cities attribute and concat the matrix \n",
        "        2. Linear + Attention (knn based) + Activation \n",
        "        3. mlp classification head\n",
        "        \"\"\"\n",
        "\n",
        "        city_embedding = self.city_embed(cities)\n",
        "        features = torch.concat((x, city_embedding), dim=-1)  # [Batch, num, dim + embed]\n",
        "        f1 = self.fc1(features)\n",
        "        attn1 = self.attn1(f1)\n",
        "        # attn1 = torch.permute(f1, [0, 2, 1])\n",
        "        # print(attn1.shape)\n",
        "        f1 = self.bn1(attn1)\n",
        "        f1 = torch.permute(f1, [0, 2, 1])\n",
        "        f1 = self.activ(f1)\n",
        "\n",
        "        f2 = self.fc2(f1)\n",
        "        attn2 = self.attn2(f2)\n",
        "        # attn2 = torch.permute(f2, [0, 2, 1])\n",
        "        f2 = self.bn2(attn2)\n",
        "        f2 = torch.permute(f2, [0, 2, 1])\n",
        "        f2 = self.activ(f2)\n",
        "\n",
        "        # flatten = attn2.view(x.shape[0], -1)\n",
        "\n",
        "        f4 = self.fc4(f2)\n",
        "        f4 = torch.permute(f4, [0, 2, 1])\n",
        "        f4 = self.bn4(f4)\n",
        "        f4 = torch.permute(f4, [0, 2, 1])\n",
        "        f4 = self.activ(f4)\n",
        "\n",
        "        output = self.mlp_head(f4)\n",
        "        return F.softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "test = DataProcess(\"./train_data_final\", unused_attrs=['district', 'city', 'zip code', 'region',\n",
        "                                                       'unit price of residence space',\n",
        "                                                       'exchange rate',\n",
        "                                                       'unit price of building space', 'total cost',\n",
        "\n",
        "                                                       ])\n",
        "\n",
        "l1, l2, l3, l4 = test.getdata(normalize=True)\n",
        "train_num = int(len(l1) * 0.9)\n",
        "city_num = len(np.unique(l2))\n",
        "zip_num = len(np.unique(l3))\n",
        "train_x, valid_x = l1[:train_num], l1[train_num:]\n",
        "train_city, valid_city = l2[:train_num], l2[train_num:]\n",
        "train_zips, valid_zips = l3[:train_num], l3[train_num:]\n",
        "train_label, valid_label = l4[:train_num], l4[train_num:]\n",
        "\n",
        "train_dataset = HousepriceDataset_V2(train_x, train_city, train_label, train_zips, 40)\n",
        "test_dataset = HousepriceDataset_V2(valid_x, valid_city, valid_label, valid_zips, 20)\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True,\n",
        "                              )\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True,\n",
        "                             )\n",
        "model = DenseNet(l1.shape[1], 8, 32, city_num, zip_num)\n",
        "\n",
        "train(train_dataloader, model, test_dataloader)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fN5wV_xs83hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "class DataProcess:\n",
        "    def __init__(self, filepath, unused_attrs):\n",
        "        self.data = None\n",
        "        self.df = None\n",
        "        self.cities = None\n",
        "        self.labels = None\n",
        "        self.total_cost = None\n",
        "        self.fp = filepath\n",
        "        self.unused = unused_attrs\n",
        "\n",
        "    def read_data(self):\n",
        "        self.df = pd.read_csv(self.fp)\n",
        "\n",
        "    def filter(self):\n",
        "        \"\"\"\n",
        "        We don't need attributes like 'District' & 'ZipCode' since they're mostly likely to be redundant,\n",
        "        in addition to this, they're hard to deal with\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.cities = self.df[\"city\"]\n",
        "        self.total_cost = self.df['total cost']\n",
        "        self.df = self.df.drop(columns=self.unused)\n",
        "\n",
        "    def encode(self, normalize:bool):\n",
        "        \"\"\"\n",
        "        1. for date attribute : extract month as feature\n",
        "        2. for city attribute : encoded through nn.Embedding, but first we need to convert them to numeric values\n",
        "        3. for label attribute : follow the rules\n",
        "        4. Feature Normalization\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # for each row in 'date' attribute, split the string and get the second value(as integer) which is month\n",
        "        if 'date' in self.df.columns:\n",
        "          self.df['date'] = self.df['date'].apply(lambda x: int(x.split('/')[1]))\n",
        "          # self.df['Month'] = self.df['date'].apply(lambda x: int(x.split('/')[1]))\n",
        "          # self.df.drop(columns=['date'])\n",
        "\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        self.cities = le.fit_transform(self.cities)\n",
        "\n",
        "        def classify(totalCost):\n",
        "            if totalCost < 300000:\n",
        "                return 1\n",
        "            elif totalCost < 500000:\n",
        "                return 2\n",
        "            elif totalCost < 700000:\n",
        "                return 3\n",
        "            else:\n",
        "                return 4\n",
        "\n",
        "        self.labels = self.total_cost.apply(lambda x: classify(x))\n",
        "        self.labels = self.labels.to_numpy(dtype=np.int32)\n",
        "        self.labels -= 1\n",
        "\n",
        "        self.data = self.df.to_numpy()\n",
        "        \"\"\"\n",
        "        it may not be as good as we think. because all values are too small\n",
        "        we may consider that only do normalization for necessary columns???\n",
        "        \"\"\"\n",
        "        if normalize:\n",
        "            self.data = preprocessing.normalize(self.data, axis=0)\n",
        "\n",
        "    def getdata(self, normalize:bool):\n",
        "        \"\"\"\n",
        "        :return:  Normalized training data, city attribute which needs to be embedded, labels\n",
        "        \"\"\"\n",
        "        self.read_data()\n",
        "        self.filter()\n",
        "        self.encode(normalize)\n",
        "        return self.data, self.cities, self.labels\n"
      ],
      "metadata": {
        "id": "ZGEUTMv88-RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.float32\n",
        "\n",
        "def train(dataloader, model, testloader, train_loss_list, train_acc_list, test_loss_list, test_acc_list):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    total_step = len(dataloader)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "    epochs = 1000\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        for i, (x, y, z) in enumerate(dataloader):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            # print(y.shape)\n",
        "            z = z.to(device)\n",
        "            predict = model(x, y)  # output shape is [Batch, num, classes]\n",
        "            predict = torch.permute(predict, [0, 2, 1])\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(predict, z)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i % 10 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, i + 1, total_step,\n",
        "                                                                         loss.item()))\n",
        "                \n",
        "                eval(model, dataloader, True)\n",
        "                _, accuracy = eval(model, testloader, False)\n",
        "                if accuracy > best_acc:\n",
        "                  best_acc = accuracy\n",
        "                  torch.save(model, \"./7.pth\")\n",
        "                print(\"Best Valid Accuracy: {}\".format(best_acc))\n",
        "        train_loss, train_acc = eval(model, dataloader, True)\n",
        "        test_loss, test_acc = eval(model, testloader, False)\n",
        "        train_loss_list.append(train_loss)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_loss_list.append(test_loss)\n",
        "        test_acc_list.append(test_acc)\n",
        "        # scheduler.step()\n",
        "\n",
        "def eval(model, testloader, train:bool):\n",
        "    \"\"\"\n",
        "    since test data doesn't have labels at all, directly use train data to measure accuracy\n",
        "    \"\"\"\n",
        "    model.eval();\n",
        "    totalNum = 0\n",
        "    correctNum = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = 0.0\n",
        "    with torch.no_grad():\n",
        "      for i, (x, y, z) in enumerate(testloader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        z = z.to(device)\n",
        "        predict = model(x, y)  # [Batch, sequence_len, classes]\n",
        "        predicts = torch.permute(predict, [0, 2, 1])\n",
        "        loss = criterion(predicts, z)\n",
        "        losses += loss.item()\n",
        "        '''\n",
        "        for j in range(x.shape[0]):\n",
        "          totalNum += x.shape[1]\n",
        "          batch_pred, batch_gt = predict[j], z[j]\n",
        "          predicted = torch.argmax(batch_pred, dim=-1)\n",
        "          sum = torch.sum(predicted == batch_gt)\n",
        "          correctNum += sum\n",
        "        '''\n",
        "        totalNum += x.shape[0] * x.shape[1]\n",
        "        predict = torch.argmax(predict, dim=-1)\n",
        "        correctNum += torch.sum(predict == z)\n",
        "    acc = correctNum / totalNum\n",
        "    losses /= len(testloader)\n",
        "    # print(totalNum)\n",
        "    if train:\n",
        "      print(\"Train Accuracy: {}\".format(acc))\n",
        "    else:\n",
        "      print(\"Valid Accuracy: {}\".format(acc))\n",
        "    return losses, acc\n",
        "    "
      ],
      "metadata": {
        "id": "I2owpz478-Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "test = DataProcess(\"./train_data_final\", unused_attrs=['district', 'city', 'zip code', 'region',\n",
        "                                                                 'unit price of residence space',\n",
        "                                                                 'unit price of building space', 'total cost',\n",
        "                                                                 \n",
        "                                                                 ])\n",
        "\n",
        "l1, l2, l3 = test.getdata(normalize=True)\n",
        "train_num = int(len(l1) * 0.9)\n",
        "city_num = len(np.unique(l2))\n",
        "l1 = pd.read_csv(\"./train_data18.csv\").to_numpy()\n",
        "l1 = l1[:, :-4]\n",
        "l1 = preprocessing.normalize(l1, axis=0)\n",
        "train_x, valid_x = l1[:train_num], l1[train_num:]\n",
        "train_city, valid_city = l2[:train_num], l2[train_num:]\n",
        "train_label, valid_label = l3[:train_num], l3[train_num:]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "因为现在的数据集文件 total cost没补充, 值是nan，所以label均为4！\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "net = DenseNet(100, 64, 64, 10)\n",
        "\n",
        "cities = torch.randint(9, size=(10, 200))\n",
        "inputx = torch.randn((10, 200, 100))\n",
        "output = net(inputx, cities)\n",
        "print(output.shape)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_dataset = ValidationDataset(train_x, train_city, train_label, 40)\n",
        "test_dataset = ValidationDataset(valid_x, valid_city, valid_label, 20)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True,\n",
        "                        )\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True,\n",
        "                        )\n",
        "model = DenseNet(l1.shape[1], 8, 32, city_num)\n",
        "\n",
        "\n",
        "trl, tra, tel, tea = list(), list(), list(), list()\n",
        "train(train_dataloader, model, test_dataloader, trl, tra, tel, tea)\n",
        "\n"
      ],
      "metadata": {
        "id": "_5XrZrI_9Iq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVftGtVR-0KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(tea)):\n",
        "  tea[i] = float(tea[i])\n",
        "  tra[i] = float(tra[i])\n",
        "tea"
      ],
      "metadata": {
        "id": "0x1sib0E9Ivq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('attention_mlp_augmentation.csv', 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow([\"Train Loss\", \"Test Loss\", \"Train Accuracy\", \"Test Accuracy\"])\n",
        "\n",
        "  for i in range(1000):\n",
        "    writer.writerow([trl[i], tel[i], tra[i], tea[i]])"
      ],
      "metadata": {
        "id": "nTNgphMrCbCo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}